from tensorflow.keras import applications
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
import random
import datetime
import os
import shutil
import math

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image

dir_path = ''  # file path of the image dataset that will used for the training
classes = os.listdir(dir_path)
len(classes)

train_dir = ''  # file path of the training image dataset
test_dir = ''  # file path of the testing image dataset

# Copying 15 random images from train folders to test folders


def prep_test_data(pokemon, train_dir, test_dir):
    pop = os.listdir(train_dir+'/'+pokemon)
    test_data = random.sample(pop, 15)
    print(test_data)
    for f in test_data:
        shutil.copy(train_dir+'/'+pokemon+'/'+f, test_dir+'/'+pokemon+'/')


# performing samething for each folder in train folder
for poke in os.listdir(train_dir):
    prep_test_data(poke, train_dir, test_dir)

target_classes = os.listdir(train_dir)
num_classes = len(target_classes)
print('Number of target classes:', num_classes)
print(list(enumerate(target_classes)))

with open('target_classes.pickle', 'wb') as handle:
    pickle.dump(list(enumerate(target_classes)), handle,
                protocol=pickle.HIGHEST_PROTOCOL)

training_set_distribution = [
    len(os.listdir(os.path.join(train_dir, dir))) for dir in os.listdir(train_dir)]
testing_set_distribution = [
    len(os.listdir(os.path.join(test_dir, dir))) for dir in os.listdir(test_dir)]


def show_pokemon(pokemon):
    num = len(pokemon)
    if num == 0:
        return None
    rows = int(math.sqrt(num))
    cols = (num+1)//rows
    f, axs = plt.subplots(rows, cols)
    fig = 0
    for b in pokemon:
        img = image.load_img(b)
        row = fig // cols
        col = fig % cols
        axs[row, col].imshow(img)
        fig += 1
    plt.show()


dir_name = os.path.join(train_dir, "Pikachu")
all_images = [os.path.join(dir_name, fname) for fname in os.listdir(dir_name)]
show_pokemon(all_images[:6])

image_size = (64, 64, 3)
datagen = ImageDataGenerator(rescale=1./255,
                             shear_range=0.2,
                             zoom_range=0.2,
                             horizontal_flip=True,
                             )

training_set = datagen.flow_from_directory(train_dir,
                                           target_size=image_size[:2],
                                           batch_size=32,
                                           class_mode='categorical',
                                           color_mode='rgb'
                                           )

validation_set = datagen.flow_from_directory(test_dir,
                                             target_size=image_size[:2],
                                             batch_size=32,
                                             class_mode='categorical',
                                             color_mode='rgb'
                                             )


es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)
filepath = "model.h5"
ckpt = ModelCheckpoint(filepath, monitor='val_loss',
                       verbose=1, save_best_only=True, mode='min')
rlp = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1)

# defining model


def cnn(image_size, num_classes):
    classifier = Sequential()
    classifier.add(Conv2D(64, (5, 5), input_shape=image_size,
                   activation='relu', padding='same'))
    classifier.add(MaxPooling2D(pool_size=(2, 2)))
    classifier.add(Flatten())
    classifier.add(Dense(num_classes, activation='softmax'))
    classifier.compile(
        optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
    return classifier


neuralnetwork_cnn = cnn(image_size, num_classes)
neuralnetwork_cnn.summary()

history = neuralnetwork_cnn.fit(
    generator=training_set, validation_data=validation_set,
    callbacks=[es, ckpt, rlp], epochs=20,
)
